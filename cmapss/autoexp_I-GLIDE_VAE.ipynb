{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8783613-6f2d-4844-a89f-95b34a1028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f15c8c-ee44-4225-b7be-7a6496cc6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensor_groups = {\n",
    "    'fan': [7, 12, 0, 4, 17, 18],\n",
    "    'LPC': [1],\n",
    "    'HPC': [2, 6, 10], \n",
    "    'core': [8, 13], \n",
    "    'turbine': [3, 19, 20], \n",
    "    'other': [11, 14, 16, 5, 9, 15] \n",
    "}\n",
    "\n",
    "class GroupEncoder(nn.Module):\n",
    "    \"\"\"Encoder module for each sensor group\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GroupDecoder(nn.Module):\n",
    "    \"\"\"Decoder module for each sensor group\"\"\"\n",
    "    def __init__(self, output_dim, latent_dim, hidden_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GroupVAE(nn.Module):\n",
    "    def __init__(self, sensor_groups, hidden_dim=10, latent_dim=2, dropout_rate=0.2, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.sensor_groups = sensor_groups\n",
    "        self.num_groups = len(sensor_groups)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Encoders for each group\n",
    "        self.encoders = nn.ModuleDict({\n",
    "            group: GroupEncoder(len(indices), hidden_dim, dropout_rate)\n",
    "            for group, indices in sensor_groups.items()\n",
    "        })\n",
    "\n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(hidden_dim * self.num_groups, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_dim * self.num_groups, latent_dim)\n",
    "\n",
    "        # Decoders for each group\n",
    "        self.decoders = nn.ModuleDict({\n",
    "            group: GroupDecoder(len(indices), latent_dim, hidden_dim, dropout_rate)\n",
    "            for group, indices in sensor_groups.items()\n",
    "        })\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode each group\n",
    "        encoded = []\n",
    "        for group, indices in self.sensor_groups.items():\n",
    "            encoded.append(self.encoders[group](x[:, indices]))\n",
    "        \n",
    "        # Concatenate and get latent parameters\n",
    "        h_concat = torch.cat(encoded, dim=1)\n",
    "        mu = self.fc_mu(h_concat)\n",
    "        log_var = self.fc_log_var(h_concat)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decode each group\n",
    "        decoded = []\n",
    "        for group in self.sensor_groups.keys():\n",
    "            decoded.append(self.decoders[group](z))\n",
    "            \n",
    "        return torch.cat(decoded, dim=1), mu, log_var\n",
    "\n",
    "    def mc_dropout_sample(self, x, num_samples=50):\n",
    "        \"\"\"Monte Carlo sampling with dropout enabled\"\"\"\n",
    "        samples = []\n",
    "        self.train()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_samples):\n",
    "                recon, _, _ = self(x)\n",
    "                samples.append(recon)\n",
    "        return torch.stack(samples)\n",
    "\n",
    "\n",
    "    def quantify_uncertainty(self, x, num_samples=50):\n",
    "        \"\"\"Returns group-wise and global uncertainty estimates\"\"\"\n",
    "        x = x.to(device)\n",
    "        mc_samples = self.mc_dropout_sample(x, num_samples)\n",
    "    \n",
    "        # Global uncertainties if you want to test with them vs by individual groups\n",
    "        reconstruction_errors_global = torch.linalg.norm(x - mc_samples, dim=2)**2  # (samples, N)\n",
    "        epistemic_global = torch.var(reconstruction_errors_global, dim=0)\n",
    "    \n",
    "        mean_recon = mc_samples.mean(dim=0)\n",
    "        aleatoric_global = torch.mean((mc_samples - mean_recon.unsqueeze(0))**2, dim=(0, 2))\n",
    "    \n",
    "        # Group-wise uncertainties\n",
    "        group_aleatoric = {}\n",
    "        group_epistemic = {}\n",
    "    \n",
    "        for group, indices in self.sensor_groups.items():\n",
    "            # Slice features for this group\n",
    "            group_samples = mc_samples[:, :, indices]\n",
    "    \n",
    "            # Compute reconstruction errors for the group\n",
    "            reconstruction_errors_group = torch.linalg.norm(x[:, indices] - group_samples, dim=2)**2  # (samples, N)\n",
    "    \n",
    "            # Calculate group-specific uncertainties\n",
    "            group_epistemic[group] = torch.var(reconstruction_errors_group, dim=0)\n",
    "            group_mean = group_samples.mean(dim=0)\n",
    "            group_aleatoric[group] = torch.mean(\n",
    "                (group_samples - group_mean.unsqueeze(0))**2,\n",
    "                dim=(0, 2)\n",
    "            )\n",
    "    \n",
    "        return {\n",
    "            'global': (aleatoric_global.cpu().numpy(), epistemic_global.cpu().numpy()),\n",
    "            'groups': (\n",
    "                {k: v.cpu().numpy() for k, v in group_aleatoric.items()},\n",
    "                {k: v.cpu().numpy() for k, v in group_epistemic.items()}\n",
    "            )\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "# Modified activation retrieval functions\n",
    "def get_group_encoder_activations(encoder, data):\n",
    "    \"\"\"Get activations from a single group encoder\"\"\"\n",
    "    activations = []\n",
    "    x = data\n",
    "    for layer in encoder.net:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            x = layer(x)\n",
    "            activations.append(x.detach())\n",
    "        else:\n",
    "            x = layer(x)\n",
    "    return activations\n",
    "\n",
    "def get_shared_latent_activation(model, data):\n",
    "    \"\"\"Get deterministic latent representation (mu)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        encoded = []\n",
    "        for group, indices in model.sensor_groups.items():\n",
    "            group_input = data[:, indices]\n",
    "            group_enc = model.encoders[group](group_input)\n",
    "            encoded.append(group_enc)\n",
    "        h_concat = torch.cat(encoded, dim=1)\n",
    "        mu = model.fc_mu(h_concat)\n",
    "    return mu.detach()\n",
    "\n",
    "\n",
    "# Corrected RaPP calculation functions\n",
    "def calculate_encoder_rapp(model, train_loader, full_data, scaler):\n",
    "    \"\"\"Calculate group-wise encoder RaPP metrics\"\"\"\n",
    "    encoder_sap = {}\n",
    "    encoder_nap = {}\n",
    "    \n",
    "    # SAP Calculation\n",
    "    with torch.no_grad():\n",
    "        X_all = torch.FloatTensor(scaler.transform(full_data)).to(device)\n",
    "        recon_all, _, _ = model(X_all)\n",
    "        \n",
    "        for group, indices in model.sensor_groups.items():\n",
    "            group_input = X_all[:, indices]\n",
    "            group_recon = recon_all[:, indices]\n",
    "            \n",
    "            orig_acts = get_group_encoder_activations(model.encoders[group], group_input)\n",
    "            recon_acts = get_group_encoder_activations(model.encoders[group], group_recon)\n",
    "            \n",
    "            sap_layers = [torch.sqrt(torch.sum((orig - rec)**2, dim=1)) \n",
    "                         for orig, rec in zip(orig_acts, recon_acts)]\n",
    "            encoder_sap[group] = torch.sum(torch.stack(sap_layers), dim=0).cpu().numpy()\n",
    "\n",
    "    # NAP Calculation\n",
    "    model.eval()\n",
    "    D_train = {group: [] for group in model.sensor_groups}\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            recon_batch, _, _ = model(batch_X)\n",
    "            \n",
    "            for group, indices in model.sensor_groups.items():\n",
    "                group_input = batch_X[:, indices]\n",
    "                group_recon = recon_batch[:, indices]\n",
    "                \n",
    "                orig_acts = get_group_encoder_activations(model.encoders[group], group_input)\n",
    "                recon_acts = get_group_encoder_activations(model.encoders[group], group_recon)\n",
    "                \n",
    "                diffs = [orig - rec for orig, rec in zip(orig_acts, recon_acts)]\n",
    "                flattened_diffs = torch.cat([d.flatten(start_dim=1) for d in diffs], dim=1)\n",
    "                D_train[group].append(flattened_diffs)\n",
    "\n",
    "    # Process each group's NAP\n",
    "    for group in model.sensor_groups:\n",
    "        D_train[group] = torch.cat(D_train[group], dim=0)\n",
    "        D_mean = D_train[group].mean(dim=0)\n",
    "        D_centered = D_train[group] - D_mean\n",
    "        \n",
    "        U, S, V = torch.linalg.svd(D_centered, full_matrices=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_full = torch.FloatTensor(scaler.transform(full_data)).to(device)\n",
    "            recon_full, _, _ = model(X_full)\n",
    "            \n",
    "            group_input = X_full[:, model.sensor_groups[group]]\n",
    "            group_recon = recon_full[:, model.sensor_groups[group]]\n",
    "            \n",
    "            orig_acts = get_group_encoder_activations(model.encoders[group], group_input)\n",
    "            recon_acts = get_group_encoder_activations(model.encoders[group], group_recon)\n",
    "            \n",
    "            diffs = [orig - rec for orig, rec in zip(orig_acts, recon_acts)]\n",
    "            D = torch.cat([d.flatten(start_dim=1) for d in diffs], dim=1)\n",
    "            D = D - D_mean\n",
    "            \n",
    "            Sigma_inv = torch.diag(1.0/(S + 1e-6))\n",
    "            proj = D @ V.T @ Sigma_inv\n",
    "            encoder_nap[group] = torch.norm(proj, dim=1).cpu().numpy()\n",
    "\n",
    "    return encoder_sap, encoder_nap\n",
    "           \n",
    "\n",
    "def calculate_latent_rapp(model, train_loader, full_data, scaler):\n",
    "    \"\"\"Calculate shared latent space RaPP metrics\"\"\"\n",
    "    # SAP Calculation\n",
    "    with torch.no_grad():\n",
    "        X_all = torch.FloatTensor(scaler.transform(full_data)).to(device)\n",
    "        mu_original = get_shared_latent_activation(model, X_all)\n",
    "        recon_all, _, _ = model(X_all)\n",
    "        mu_recon = get_shared_latent_activation(model, recon_all)\n",
    "        \n",
    "    sap = torch.sqrt(torch.sum((mu_original - mu_recon)**2, dim=1)).cpu().numpy()\n",
    "\n",
    "    # NAP Calculation\n",
    "    model.eval()\n",
    "    D_train = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            mu_orig = get_shared_latent_activation(model, batch_X)\n",
    "            recon_batch, _, _ = model(batch_X)\n",
    "            mu_recon = get_shared_latent_activation(model, recon_batch)\n",
    "            D_train.append(mu_orig - mu_recon)\n",
    "    \n",
    "    D_train = torch.cat(D_train, dim=0)\n",
    "    D_mean = D_train.mean(dim=0)\n",
    "    D_centered = D_train - D_mean\n",
    "    \n",
    "    U, S, V = torch.linalg.svd(D_centered, full_matrices=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_full = torch.FloatTensor(scaler.transform(full_data)).to(device)\n",
    "        mu_orig = get_shared_latent_activation(model, X_full)\n",
    "        recon_full, _, _ = model(X_full)\n",
    "        mu_recon = get_shared_latent_activation(model, recon_full)\n",
    "        \n",
    "        D = (mu_orig - mu_recon) - D_mean\n",
    "        \n",
    "        Sigma_inv = torch.diag(1.0/(S + 1e-6))\n",
    "        proj = D @ V.T @ Sigma_inv\n",
    "        nap = torch.norm(proj, dim=1).cpu().numpy()\n",
    "\n",
    "    return sap, nap\n",
    "\n",
    "\n",
    "# Compute RUL for each unit\n",
    "# Fixed RUL calculation function\n",
    "def calculate_rul(data):\n",
    "    # Get unique units using dataset_id (column 0) and unit_num (column 1)\n",
    "    unique_units = np.unique(data[:, [0, 1]], axis=0)\n",
    "    \n",
    "    for dataset_id, unit_num in unique_units:\n",
    "        # Create mask for this specific unit\n",
    "        mask = (data[:, 0] == dataset_id) & (data[:, 1] == unit_num)\n",
    "        unit = data[mask]\n",
    "        \n",
    "        # Skip if no data found (shouldn't happen with proper input)\n",
    "        if unit.size == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate RUL components\n",
    "        max_cycle = np.max(unit[:, 2])  # Max cycle for this unit\n",
    "        rul_extra = dataset_test_RUL[int(unit_num)-1] if dataset_id == 1 else 0\n",
    "        rul_values = (max_cycle - unit[:, 2]) + rul_extra\n",
    "        \n",
    "        # Update RUL in original data\n",
    "        data[mask, 2] = rul_values\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Process datasets to retain unit numbers\n",
    "def process_dataset(data, unit_id=None):\n",
    "    if unit_id is not None:\n",
    "        # Filter the data to include only the entries with the specified unit_id\n",
    "        data = data[data[:, 0] == unit_id]\n",
    "\n",
    "    return np.hstack((\n",
    "        data[:, 0].reshape(-1, 1),   # Unit number\n",
    "        data[:, 1].reshape(-1, 1),   # Cycles\n",
    "        data[:, 5:]                  # Sensors\n",
    "    ))\n",
    "\n",
    "# Create sliding windows\n",
    "def create_windows(data, window_size=30, step=1, threshold=125):\n",
    "    windows = []\n",
    "    unique_units = np.unique(data[:, :2], axis=0)\n",
    "    for dataset_id, unit_num in unique_units:\n",
    "        mask = (data[:, 0] == dataset_id) & (data[:, 1] == unit_num)\n",
    "        unit_data = data[mask]\n",
    "        n_samples = len(unit_data)\n",
    "        for i in range(0, n_samples - window_size + 1, step):\n",
    "            window = unit_data[i:i+window_size, 3:]\n",
    "            target_rul = unit_data[i+window_size-1, 2]\n",
    "            if target_rul > threshold:\n",
    "                windows.append(window.flatten())\n",
    "    return np.array(windows)\n",
    "    \n",
    "\n",
    "# 1. Corrected data processing without windowing\n",
    "def create_full_samples(data, is_test=False, clip_threshold=None):\n",
    "    samples = []\n",
    "    ruls = []\n",
    "    unique_units = np.unique(data[:, 0])  # Get unique unit numbers\n",
    "\n",
    "    for unit_num in unique_units:\n",
    "        mask = data[:, 0] == unit_num\n",
    "        unit_data = data[mask]\n",
    "\n",
    "        max_cycle = np.max(unit_data[:, 1])  # Max cycle for this unit\n",
    "        rul_extra = dataset_test_RUL[int(unit_num)-1] if is_test else 0\n",
    "\n",
    "        for i in range(len(unit_data)):\n",
    "            sample = unit_data[i, 2:]  # Sensor measurements\n",
    "            target_rul = (max_cycle - unit_data[i, 1]) + rul_extra\n",
    "\n",
    "            if clip_threshold is not None:\n",
    "                target_rul = min(target_rul, clip_threshold)\n",
    "\n",
    "            samples.append(sample)\n",
    "            ruls.append(target_rul)\n",
    "\n",
    "    return np.array(samples), np.array(ruls)\n",
    "\n",
    "# 2. Create features from original model\n",
    "def create_feature_df(model, X, ruls, train_loader, scaler):\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(scaler.transform(X))\n",
    "        uncertainties = model.quantify_uncertainty(X_tensor)\n",
    "    \n",
    "    # Unpack uncertainties\n",
    "    aleatoric_global, epistemic_global = uncertainties['global']\n",
    "    aleatoric_groups, epistemic_groups = uncertainties['groups']\n",
    "\n",
    "    encoder_sap, encoder_nap = calculate_encoder_rapp(model, train_loader, X, scaler)\n",
    "    latent_sap, latent_nap = calculate_latent_rapp(model, train_loader, X, scaler)\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Aleatoric': aleatoric_global,\n",
    "        'Epistemic': epistemic_global,\n",
    "        'Latent_SAP': latent_sap,\n",
    "        'Latent_NAP': latent_nap,\n",
    "        'RUL': ruls\n",
    "    })\n",
    "\n",
    "    # Add encoder-specific metrics\n",
    "    for group in sensor_groups.keys():\n",
    "        metrics_df[f'Encoder_{group}_SAP'] = encoder_sap[group]\n",
    "        metrics_df[f'Encoder_{group}_NAP'] = encoder_nap[group]\n",
    "        metrics_df[f'Aleatoric_{group}'] = aleatoric_groups[group]\n",
    "        metrics_df[f'Epistemic_{group}'] = epistemic_groups[group]\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def train_evaluate_rul_model(X_train_copy, y_train_copy, X_test_copy, y_test_copy, patterns_to_keep, RUL_PARAMS, RF_PARAMS):\n",
    "    \"\"\"\n",
    "    Train and evaluate a RandomForestRegressor model for RUL prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_copy: DataFrame, training features\n",
    "    - y_train_copy: Series, training target\n",
    "    - X_test_copy: DataFrame, test features\n",
    "    - y_test_copy: Series, test target\n",
    "    - patterns_to_keep: List of strings, patterns to keep in the features\n",
    "    - RUL_PARAMS: Dictionary, parameters for RUL processing\n",
    "    - RF_PARAMS: Dictionary, parameters for RandomForestRegressor\n",
    "\n",
    "    Returns:\n",
    "    - trained_model: Trained RandomForestRegressor model\n",
    "    - mse: Mean Squared Error on the full test set\n",
    "    - mae: Mean Absolute Error on the full test set\n",
    "    - mse_last: Mean Squared Error on the test_last set\n",
    "    - mae_last: Mean Absolute Error on the test_last set\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter columns based on patterns\n",
    "    columns_to_keep_train = [col for col in X_train_copy.columns if any(pattern in col for pattern in patterns_to_keep)]\n",
    "    X_train = X_train_copy[columns_to_keep_train].copy()\n",
    "    columns_to_keep_test = [col for col in X_test_copy.columns if any(pattern in col for pattern in patterns_to_keep)]\n",
    "    X_test = X_test_copy[columns_to_keep_test].copy()\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    def train_evaluate_model(X_train, y_train, X_test, y_test):\n",
    "        # Handle RUL modifications\n",
    "        if RUL_PARAMS['clip_threshold']:\n",
    "            y_train = np.clip(y_train, None, RUL_PARAMS['clip_threshold'])\n",
    "            y_test = np.clip(y_test, None, RUL_PARAMS['clip_threshold'])\n",
    "            # print(f\"Clipped RUL values at {RUL_PARAMS['clip_threshold']}\")\n",
    "\n",
    "        if RUL_PARAMS['filter_threshold']:\n",
    "            train_mask = y_train < RUL_PARAMS['filter_threshold']\n",
    "            test_mask = y_test < RUL_PARAMS['filter_threshold']\n",
    "            X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "            X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "            # print(f\"Filtered RUL >= {RUL_PARAMS['filter_threshold']}\")\n",
    "\n",
    "        # Initialize model\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=RF_PARAMS['n_estimators'],\n",
    "            max_depth=RF_PARAMS['max_depth'],\n",
    "            random_state=RF_PARAMS['random_state']\n",
    "        )\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        if RUL_PARAMS['tune_hyperparams']:\n",
    "            # print(\"Tuning hyperparameters...\")\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=rf,\n",
    "                param_grid=RF_PARAMS['param_grid'],\n",
    "                cv=5,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_rf = grid_search.best_estimator_\n",
    "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        else:\n",
    "            # print(\"Using default parameters\")\n",
    "            best_rf = rf\n",
    "            best_rf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on the full test set\n",
    "        y_pred = best_rf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Create test_last dataset\n",
    "        test_last_indices = np.where(np.diff(y_test, prepend=y_test[0]+1) > 0)[0]\n",
    "        X_test_last = X_test[test_last_indices]\n",
    "        y_test_last = y_test[test_last_indices]\n",
    "\n",
    "        # Evaluate on the test_last set\n",
    "        y_pred_last = best_rf.predict(X_test_last)\n",
    "        mse_last = mean_squared_error(y_test_last, y_pred_last)\n",
    "        mae_last = mean_absolute_error(y_test_last, y_pred_last)\n",
    "        r2_last = r2_score(y_test_last, y_pred_last)\n",
    "\n",
    "        return best_rf, mse, mae, mse_last, mae_last\n",
    "\n",
    "    # Run training and evaluation\n",
    "    trained_model, mse, mae, mse_last, mae_last = train_evaluate_model(\n",
    "        X_train_scaled,\n",
    "        y_train_copy.values,\n",
    "        X_test_scaled,\n",
    "        y_test_copy.values\n",
    "    )\n",
    "\n",
    "    # Feature importance visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': trained_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    # print(\"\\nFeature Importances:\")\n",
    "    # print(feature_importance)\n",
    "\n",
    "    print(f'| {RUL_PARAMS['clip_threshold']} | {RUL_PARAMS['filter_threshold']} | {math.sqrt(mse):.2f} | {mae:.2f} | {math.sqrt(mse_last):.2f} | {mae_last:.2f} | {feature_importance.iloc[0]['Feature']} {feature_importance.iloc[0]['Importance']:.2f} | {feature_importance.iloc[1]['Feature']} {feature_importance.iloc[1]['Importance']:.2f} |', end='')\n",
    "    if len(feature_importance) > 2:\n",
    "        print(f'{feature_importance.iloc[2]['Feature']} {feature_importance.iloc[2]['Importance']:.2f} |')\n",
    "    else:\n",
    "        print(f'- |')\n",
    "\n",
    "    rmse = math.sqrt(mse)\n",
    "    rmse_last = math.sqrt(mse_last)\n",
    "\n",
    "    return trained_model, rmse, rmse_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdecec20-5aaa-4120-8163-caa39fb786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_experiment_run(make_model=False):\n",
    "    \n",
    "    # Load datasets\n",
    "    dataset_train = np.loadtxt(f'data/train_{dataset_name}.txt')\n",
    "    dataset_test = np.loadtxt(f'data/test_{dataset_name}.txt')\n",
    "    dataset_test_RUL = np.loadtxt(f'data/RUL_{dataset_name}.txt')\n",
    "    \n",
    "    \n",
    "    train_data = process_dataset(dataset_train)\n",
    "    test_data = process_dataset(dataset_test)\n",
    "    \n",
    "    full_data = np.vstack((\n",
    "        np.hstack((np.zeros((train_data.shape[0], 1)), train_data)),\n",
    "    ))\n",
    "\n",
    "    full_data_test = np.vstack((\n",
    "        np.hstack((np.zeros((test_data.shape[0], 1)), test_data)),\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    # Apply corrected RUL calculation\n",
    "    full_data = calculate_rul(full_data)\n",
    "    full_data_test = calculate_rul(full_data_test)\n",
    "\n",
    "    \n",
    "    window_size = 1\n",
    "    R_early_train = 80\n",
    "    X = create_windows(full_data, window_size=window_size, threshold=R_early_train)\n",
    "    X_test = create_windows(full_data_test, window_size=window_size, threshold=R_early_train)\n",
    "    \n",
    "    # Train-validation split\n",
    "    X_train = X\n",
    "    X_val = X_test\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    len(train_data), len(X_train_scaled)\n",
    "    \n",
    "    ### INIT MODEL\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    model = GroupVAE(\n",
    "        sensor_groups=sensor_groups,\n",
    "        hidden_dim=2,\n",
    "        latent_dim=3,\n",
    "        dropout_rate=0.2,\n",
    "        device='cpu'\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    \n",
    "    ### TRAIN MODEL\n",
    "    # Convert to tensors\n",
    "    train_data = TensorDataset(torch.FloatTensor(X_train_scaled), \n",
    "                              torch.FloatTensor(X_train_scaled))\n",
    "    val_data = TensorDataset(torch.FloatTensor(X_val_scaled),\n",
    "                            torch.FloatTensor(X_val_scaled))\n",
    "    \n",
    "    # Data loaders\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)#, num_workers=12)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)#, num_workers=12)\n",
    "\n",
    "    model.to(device)\n",
    "    # Training\n",
    "    for epoch in range(200):\n",
    "        # KL annealing \n",
    "        current_KL_coef = min(1.0, epoch / 50)  # Now actually using annealing\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_recon = 0\n",
    "        train_kl = 0\n",
    "        \n",
    "        for batch_X, _ in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - same interface as original VAE\n",
    "            recon_batch, mu, log_var = model(batch_X)\n",
    "            \n",
    "            # Reconstruction loss (same as before)\n",
    "            recon_loss = criterion(recon_batch, batch_X)\n",
    "            \n",
    "            # KL Divergence calculation (adjusted for correct scaling)\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / batch_X.size(0)\n",
    "            \n",
    "            # Combined loss with annealing\n",
    "            loss = recon_loss + kl_loss * current_KL_coef\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            train_loss += loss.item()\n",
    "            train_recon += recon_loss.item()\n",
    "            train_kl += kl_loss.item()\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_recon = 0\n",
    "        val_kl = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, _ in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                recon_val, mu_val, log_var_val = model(batch_X)\n",
    "                \n",
    "                # Validation reconstruction loss\n",
    "                recon_loss_val = criterion(recon_val, batch_X)\n",
    "                \n",
    "                # KL loss (same calculation as training)\n",
    "                kl_loss_val = -0.5 * torch.sum(1 + log_var_val - mu_val.pow(2) - log_var_val.exp()) / batch_X.size(0)\n",
    "                \n",
    "                # Validation loss (using same KL coefficient)\n",
    "                loss_val = recon_loss_val + kl_loss_val * current_KL_coef\n",
    "                \n",
    "                val_loss += loss_val.item()\n",
    "                val_recon += recon_loss_val.item()\n",
    "                val_kl += kl_loss_val.item()\n",
    "    \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "    \n",
    "        # Calculate epoch metrics\n",
    "        train_recon_avg = train_recon / len(train_loader)\n",
    "        train_kl_avg = train_kl / len(train_loader)\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "    \n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_recon_avg = val_recon / len(val_loader)\n",
    "        val_kl_avg = val_kl / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1:03d}: '\n",
    "          f'Train Loss: {train_loss_avg:.4f} (Recon: {train_recon_avg:.4f}, KL: {train_kl_avg:.4f}) | '\n",
    "          f'Val Loss: {val_loss_avg:.4f} (Recon: {val_recon_avg:.4f}, KL: {val_kl_avg:.4f})')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    ### CREATE OUR METRICS FOR RANDOM FOREST\n",
    "    # Load datasets\n",
    "    dataset_train = np.loadtxt(f'data/train_{dataset_name}.txt')\n",
    "    dataset_test = np.loadtxt(f'data/test_{dataset_name}.txt')\n",
    "    dataset_test_RUL = np.loadtxt(f'data/RUL_{dataset_name}.txt')\n",
    "    \n",
    "    train_data = process_dataset(dataset_train)\n",
    "    test_data = process_dataset(dataset_test)\n",
    "    \n",
    "    # 3. Full pipeline\n",
    "    # Process raw data without thresholds\n",
    "    train_samples, train_ruls = create_full_samples(train_data, is_test=False)\n",
    "    test_samples, test_ruls = create_full_samples(test_data, is_test=True)\n",
    "    \n",
    "    # Scaling (use same scaler as before)\n",
    "    scaler = MinMaxScaler().fit(train_samples)\n",
    "    \n",
    "    # Create feature DataFrames\n",
    "    train_feats_df = create_feature_df(model, train_samples, train_ruls, train_loader, scaler)\n",
    "    test_feats_df = create_feature_df(model, test_samples, test_ruls, train_loader, scaler)\n",
    "    \n",
    "    # 4. Train/test split\n",
    "    X_train = train_feats_df.drop('RUL', axis=1)\n",
    "    y_train = train_feats_df['RUL']\n",
    "    X_test = test_feats_df.drop('RUL', axis=1)\n",
    "    y_test = test_feats_df['RUL']\n",
    "    \n",
    "    init = True\n",
    "    \n",
    "    if init:\n",
    "        X_train_copy = X_train.copy()\n",
    "        y_train_copy = y_train.copy()\n",
    "        X_test_copy = X_test.copy()\n",
    "        y_test_copy = y_test.copy()\n",
    "        init = False\n",
    "    \n",
    "    # Now train multiple RFs\n",
    "    RUL_PARAMS = {\n",
    "        'clip_threshold': 125,    # Set to None to disable clipping\n",
    "        'filter_threshold': False, # Set to value (e.g. 125) to filter RUL >= threshold\n",
    "        'tune_hyperparams': True  # Set to False to use default parameters\n",
    "    }\n",
    "    RF_PARAMS = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 15,\n",
    "    'random_state': 42,\n",
    "    'param_grid': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [10],\n",
    "        'min_samples_split': [2]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Define the parameters to iterate over\n",
    "    filter_thresholds = [False]\n",
    "    patterns_list = [\n",
    "        ['Encoder', 'Latent', 'Aleatoric_', 'Epistemic_'],\n",
    "        ['Latent'],\n",
    "        ['Encoder'],\n",
    "        ['Aleatoric', 'Epistemic']\n",
    "    ]\n",
    "    \n",
    "    # Initialize a DataFrame to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Loop over the parameter combinations\n",
    "    for filter_threshold in filter_thresholds:\n",
    "        for patterns_to_keep in patterns_list:\n",
    "            # Update RUL_PARAMS\n",
    "            RUL_PARAMS_copy = RUL_PARAMS.copy()\n",
    "            RUL_PARAMS_copy['filter_threshold'] = filter_threshold\n",
    "    \n",
    "            # Train and evaluate the model\n",
    "            trained_model, rmse, rmse_last = train_evaluate_rul_model(\n",
    "                X_train_copy, y_train_copy, X_test_copy, y_test_copy, patterns_to_keep, RUL_PARAMS_copy, RF_PARAMS\n",
    "            )\n",
    "    \n",
    "            # Store the results\n",
    "            results.append({\n",
    "                'filter_threshold': filter_threshold,\n",
    "                'patterns_to_keep': patterns_to_keep,\n",
    "                'rmse': rmse,\n",
    "                'rmse_last': rmse_last,\n",
    "            })\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba4bd9-1791-4671-8315-974d418ac086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_names = ['FD001']#, 'FD002', 'FD003', 'FD004']\n",
    "device = 'cpu'\n",
    "model_name = 'I-GLIDE_VAE'\n",
    "num_runs = 10\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f'Processing dataset {dataset_name}')\n",
    "\n",
    "    dataset_test_RUL = np.loadtxt(f'data/RUL_{dataset_name}.txt')\n",
    "    csv_path = f'benchmarks/{model_name}_{dataset_name}.csv'\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    \n",
    "    \n",
    "    all_exp_results = []\n",
    "    \n",
    "    for experiment_run in range(num_runs):\n",
    "        print(f'## Doing run {experiment_run+1}')\n",
    "        results = make_experiment_run()\n",
    "        all_exp_results.append(results)\n",
    "    \n",
    "    # Concatenate all results into a single DataFrame\n",
    "    all_results_df = pd.concat(all_exp_results, ignore_index=True)\n",
    "    \n",
    "    # Save or append results to the CSV file\n",
    "    if os.path.exists(csv_path):\n",
    "        # Append to the existing file\n",
    "        all_results_df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Create a new file\n",
    "        all_results_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f'Results saved to {csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e032d58-69ba-47aa-90c2-146f427e8261",
   "metadata": {},
   "source": [
    "# Make some plots of the HIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329ab62-fbb3-49db-8022-5aa94e956a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a HI model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_name = 'FD001'\n",
    "device = 'cpu'\n",
    "num_runs = 1\n",
    "\n",
    "print(f'Processing dataset {dataset_name}')\n",
    "\n",
    "dataset_test_RUL = np.loadtxt(f'data/RUL_{dataset_name}.txt')\n",
    "model = make_experiment_run(make_model=True)\n",
    "all_exp_results.append(results)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1cd29d-5866-446a-bbd2-ac798cc71f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = np.loadtxt(f'data/train_{dataset_name}.txt')\n",
    "dataset_test = np.loadtxt(f'data/test_{dataset_name}.txt')\n",
    "dataset_test_RUL = np.loadtxt(f'data/RUL_{dataset_name}.txt')\n",
    "batch_size=1\n",
    "\n",
    "# 1. Initial dataloaders to compute NAP\n",
    "train_data = process_dataset(dataset_train)\n",
    "test_data = process_dataset(dataset_test)\n",
    "\n",
    "full_data = np.vstack((\n",
    "    np.hstack((np.zeros((train_data.shape[0], 1)), train_data)),\n",
    "))\n",
    "\n",
    "full_data_test = np.vstack((\n",
    "    np.hstack((np.zeros((test_data.shape[0], 1)), test_data)),\n",
    "))\n",
    "\n",
    "\n",
    "# Apply corrected RUL calculation\n",
    "full_data = calculate_rul(full_data)\n",
    "full_data_test = calculate_rul(full_data_test)\n",
    "\n",
    "window_size = 1\n",
    "R_early_train = 125\n",
    "X = create_windows(full_data, window_size=window_size, threshold=R_early_train)\n",
    "X_test = create_windows(full_data_test, window_size=window_size, threshold=R_early_train)\n",
    "\n",
    "X_train = X\n",
    "X_val = X_test\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "len(train_data), len(X_train_scaled)\n",
    "\n",
    "### INIT MODEL\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = GroupVAE(\n",
    "    sensor_groups=sensor_groups,\n",
    "    hidden_dim=2,\n",
    "    latent_dim=3,\n",
    "    dropout_rate=0.2,\n",
    "    device='cpu'\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "### TRAIN MODEL\n",
    "# Convert to tensors\n",
    "train_data = TensorDataset(torch.FloatTensor(X_train_scaled), \n",
    "                      torch.FloatTensor(X_train_scaled))\n",
    "val_data = TensorDataset(torch.FloatTensor(X_val_scaled),\n",
    "                    torch.FloatTensor(X_val_scaled))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)#, num_workers=12)\n",
    "\n",
    "train_data = process_dataset(dataset_train)\n",
    "train_samples, train_ruls = create_full_samples(train_data, is_test=False)\n",
    "\n",
    "# Scaling (use same scaler as before)\n",
    "scaler = MinMaxScaler().fit(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f886a29-ecf5-4fba-86e3-ac830aeb2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id = 10\n",
    "train_data = process_dataset(dataset_test, unit_id=unit_id)\n",
    "\n",
    "# 3. Full pipeline\n",
    "# Process raw data without thresholds\n",
    "train_samples, train_ruls = create_full_samples(train_data, is_test=False)\n",
    "\n",
    "# Create feature DataFrames\n",
    "train_feats_df = create_feature_df(model, train_samples, train_ruls, train_loader, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4c74e-da22-40e6-a251-f642f57bc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_matching_columns(df, keywords, r_early=125, figsize=(12, 6), title='Plot of Matching Columns'):\n",
    "    # Filter the DataFrame to include only the columns that match the keywords\n",
    "    matching_columns = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n",
    "    filtered_df = df[matching_columns]\n",
    "\n",
    "    # Determine the number of entries to plot\n",
    "    if r_early is not None:\n",
    "        filtered_df = filtered_df.tail(r_early)\n",
    "\n",
    "    # Set the style using seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Set the font to Times New Roman\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['text.color'] = 'black'\n",
    "\n",
    "    # Plot the matching columns\n",
    "    filtered_df.plot(figsize=figsize)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('timestep', fontsize=14)\n",
    "    plt.ylabel('values', fontsize=14)\n",
    "    plt.legend(loc='upper left', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "plot_matching_columns(train_feats_df, ['SAP'], figsize=(6, 5), title=f'SAP Encoder HIs for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Encoder_HPC', 'Encoder_turbine', 'Encoder_fan'], figsize=(6, 5), title=f'Encoder HIs for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['fan', 'HPC'], figsize=(6, 5), title=f'Encoder HIs for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Encoder_HPC_NAP', 'Encoder_turbine_NAP', 'Encoder_fan_NAP'], figsize=(6, 5), title=f'Encoder NAP HIs for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Latent'], figsize=(6, 5), title=f'Latent HI for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Aleatoric_HPC', 'Epistemic_HPC', 'Aleatoric_turbine', 'Epistemic_turbine', 'Aleatoric_fan', 'Epistemic_fan'], figsize=(6, 5), title=f'UQ HI for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Aleatoric_'], figsize=(6, 5), title=f'Aleatoric UQ HI for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Epistemic_HPC', 'Epistemic_fan'], figsize=(6, 5), title=f'Epistemic UQ HI for engine 1 on {dataset_name}')\n",
    "plot_matching_columns(train_feats_df, ['Epistemic'], figsize=(6, 5), title=f'Epistemic UQ HI for engine 1 on {dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a57ca-1c04-4efd-ac78-576480da627d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
